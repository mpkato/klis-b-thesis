\chapter{関連研究}
\label{sq:RelatedWork}
本節では，エンティティのランキングに関する関連研究，および，
文脈誘導型学習と他の機械学習手法，特にマルチタスク学習との関係について述べる．

\section{エンティティランキング}
エンティティランキングはINEXやTRECのいくつかのトラックで取り組まれてきている．
INEXのEntity Rankingトラックでは，エンティティランキングとエンティティリスト補完の2つのタスクが提案された~\cite{de2007overview,demartini2008overview,demartini2009overview}．
エンティティランキングタスクは，与えられたクエリに対して適合するエンティティを発見するタスクであり，他方，エンティティリスト補完タスクでは与えられたエンティティ例に関連するエンティティを発見するタスクであった．
TRECのエンティティトラックでは，あるエンティティ例と発見対象のエンティティタイプ，それらの関係性が与えられたときに関連するエンティティを取得する，関連エンティティ発見タスクが提案された~\cite{balog2009overview,balog2010overview,balog2011overview}．
これらのタスクでは，発見されたエンティティが与えられたエンティティ例との関連性に基づいて順序づけされることだけが求められ，関連エンティティに対して様々な順序付けを行うことまでは求められていない．

エンティティリスト補完タスクと関連エンティティ発見タスクとは別に，
エンティティのランキング学習に関する関連研究も存在する．
Kangらは与えられたクエリに関連するエンティティを発見するために，決定木ブースティングに基づくランキングアルゴリズムを用いている~\cite{kang2015learning}．
Tranらはイベントのタイムライン要約のために主要度と情報量に基づいてエンティティをランキングする手法を提案している~\cite{tran2015balancing}．
Zhouらは与えられたエンティティと特定の関係にあるようなエンティティを発見する問題に取り組んでいる~\cite{zhou2013learning}．
例えば，ユーザは``FounderOf''という関係と``Microsoft''というエンティティを入力し``Bill Gates''を得ることができる．
彼らはあるエンティティ対の関係に関する検索スニペットから得られた特徴を用いて，
訓練クエリとラベル付けされたエンティティを元に各関係についてランキングを学習した．
この研究と我々の研究は共に文脈（または検索スニペット）を用いてランキング学習を行うという点では類似するが，
我々のモデルは主にエンティティの属性に基づくものであり，エンティティ対の文脈は用いていない．

我々のタスクに類似した自然言語処理タスクもいくつか提案されている．
Iwanariらは，与えられた形容詞に基づいてエンティティを順序づける問題に取り組んでいる~\cite{iwanari2016ordering}．
手法はテキストから得られたいくつかの根拠に基づいており，
エンティティと形容詞の共起やエンティティと形容詞の依存関係，直喩，比較文などが用いられている．
このタスクには既存のランキング学習および回帰モデルが用いられている．
ある順序基準に基づいてエンティティをランキングを行うという点でこのタスクは我々のタスクと類似する．
彼らの手法は順序基準とエンティティの文脈を利用するが，
我々の手法では順序基準とエンティティの「属性」の文脈を利用している．
また，本論文ではこのタスクについて効果的な学習手法についても提案を行っている．
エンティティの数値的な属性を抽出する研究も存在するが，
我々のタスクでの順序基準は多くの場合に定量化することが困難であり，
これらの研究で提案される手法では推定できないと思われる~\cite{takamura2015estimating,davidov2010extraction,madaan2016numerical}．

\section{マルチタスク学習}

文脈誘導型学習の重要な特徴を以下に要約する：
(1) 関数$f$の重みはラベルだけでなく，ラベル基準と特徴の文脈に基づいて学習される，また，
(2) 文脈と関数$f$中の重みの関係を学習するために，複数の関数が同時に学習される．
以下では，いくつかの機械学習手法について述べ文脈誘導型学習との関連について議論する．

マルチタスク学習とは複数のタスクを同時に学習することによって個々のタスクの学習を改善する方法である~\cite{caruana1997multitask}．
文脈誘導型学習はマルチタスク学習の一種であると考えられる．
EvgeniouとPontilによって提案された正則化マルチタスク学習は
複数のタスクにおける関数の重みが似ていることを仮定している~\cite{evgeniou2004regularized}．
後に述べるように，彼らのモデルは我々のモデルの特殊な場合であり，
すべての文脈が同じである場合に等価なモデルとなる．
また，共通の事前分布から重みが生成されることを仮定するモデルもある~\cite{yu2005learning,lee2007learning,daume2009bayesian}．
Argyriouらは重みが複数のタスクに共通する低次の部分空間において表現できることを仮定している~\cite{argyriou2008convex}．
これらのように全てのタスクが関連すると仮定する研究とは対照的に，
いくつかの研究ではどのタスクが関連し，似た重みを共有すべきか選択するようなモデルを提案している~\cite{jacob2009clustered,kumar2012learning}．
同様に，文脈誘導型学習はタスク間の類似性を文脈を用いることで暗に推定し，
似たようなタスクに対して似たような重みを推定する傾向にある．
文脈誘導型学習と他のマルチタスク学習手法の興味深い違いは，
いずれのタスクも類似しない場合においても文脈誘導型学習は適用可能であるという点である．
文脈誘導型学習は複数のタスク間でいくつかの文脈が類似することだけを仮定する．
したがって，文脈誘導型学習の適用範囲は既存のマルチタスク学習が対象とする問題に限らない．
%PanとYangが述べたように，
%マルチタスク学習の手法は転移学習用に簡単に修正することができるため（彼らのサーベイ論文の3.3節を参照~\cite{pan2010survey}），
%文脈誘導型学習は転移学習のための手法であるとも考えることができる．


最後に，文脈誘導型学習（Context-guided Learning，CGL）と似た名前を持つ機械学習手法との差違を明確にする．
文脈依存型学習（Context-{\it sensitive} Learning）はしばしば特徴の文脈（例えば，ピクセルを特徴とする場合の周辺ピクセル~\cite{bovolo2006novel}や単語を特徴とする場合の共起単語~\cite{cohen1999context}など）を用いる手法を指す．
他の場合，与えられたデータに対して選択的にモデルを用いる機械学習手法~\cite{qi2002context}や
直前・直後のデータを予測に用いる手法~\cite{metallinou2012context}を文脈依存型学習と呼ぶ．
これらの手法は主に文脈を追加の特徴として用いることに焦点を当てており，
文脈誘導型学習はモデルを変えることなく，文脈を用いてモデルの学習を改善する手法である．
さらに，文脈誘導型学習で用いられる文脈は特徴に関するものではなく，
ラベル基準と特徴に関するものである．