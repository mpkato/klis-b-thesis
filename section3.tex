\chapter{提案手法}
\label{sq:Methodology}

本節では，まず部分的に観測されたエンティティの順序から，エンティティの順序を数値属性に基づいて学習する問題について説明を行う．
それから，文脈誘導型学習を導入し，その主問題への適用について述べる．

\section{問題設定}

$E$をあるクラスのエンティティ集合とし，
エンティティ順序$\preceq_{k}$を$E$上の全順序として定義する．
各順序は順序の基準を示す順序基準$l_k$を持つ．
例えば，順序基準として「住みやすさ」，「革新性」，「美しさ」，「性能」などが挙げられる．
順序付き集合$O_{\preceq_{k}}$ ($\subset E \times E$)は$e_i \preceq_{k} e_j$を満たす全てのエンティティ対の集合$(e_i, e_j) \in E \times E$である．
Web上においてエンティティ順序はこれら順序付き集合の部分集合によって表現されることが多い．
従って，エンティティ順序の学習のために観測・利用可能なのは$O'_{\preceq_{k}} \subset O_{\preceq_{k}}$である．
例えば， ``レオナルド ディカプリオはブラッド ピットよりも背が高い''という文は
$O'_{\preceq_{k}} = \{($``ブラッド ピット'', ``レオナルド ディカプリオ''$)\}$
を示し，``1位：デンマーク，2位：スイス，3位：アイスランド''というランキングは
$O'_{\preceq_{k}} = \{($``スイス'', ``デンマーク''$)$,
$($``アイスランド'', ``デンマーク''$)$, $($``アイスランド'', ``スイス''$)\}$
を示す．

我々の主目的は，各エンティティ$\preceq_{k}$に対して順序部分的に観測された順序付き集合$O'_{\preceq_{k}}$から線形関数$f_k(e_i) = {\mathbf w}_{k}^T {\mathbf e}_i$を学習することである．ただし，${\mathbf e}_i$はエンティティ$e_i \in E$の数値属性を表す$M$次元ベクトルであり，このベクトルの$d$次元目はエンティティの$a_d$という属性を表す．
我々はこの関数$f_k$がエンティティ順序$\preceq_{k}$を{\bf 保つ}ことを期待する：
つまり，任意の$e_i, e_j \in E$に対して$e_i \preceq_{k} e_j \Rightarrow f_k(e_i) \leq f_k(e_j)$を満たすことを期待する．
学習された関数$f_k$を用いることで順序基準$l_k$によってエンティティを順位付けすることが可能であり，学習された関数中において0でない重みを持つ数値属性によって順序基準を説明することができる．

先に述べたように，この問題の主となる課題は訓練データの不足である．
より正確には，数値属性の数$M$と比較して$|O'_{\preceq_{k}}|$がしばしば小さいことである．
例えば，我々の実験において「国」では$M=83$，「都道府県」では$M=137$である．
10件以下のエンティティしかランキングされていない場合，訓練データとして我々が得られるのは高々45個のエンティティペアであり，これらのクラスにおいては学習に十分な量であるとは考えにくい．
さらに，幅広い順序を扱うためには$M$はできるだけ大きい方が良い．
したがって，訓練データの不足から起こりうる過学習を防ぐためになんらかの手段が必要となる．

本研究における主なアイデアは${\mathbf w}_{k}$の学習のために，$O'_{\preceq_{k}}$以外のデータを活用することである．
我々の問題の特徴，または，仮定として，
順序基準と属性に対するテキスト表現が利用可能であることが挙げられる．
そのため，順序基準と属性に関する文脈を活用することによって，
各属性に対する重みをおおよそ推定することができるのではないかと我々は考えた．
例えば，``{\bf GDP}が向上すれば{\bf 幸福度}も改善する''という文脈からは「GDP」に対する正の重みを，
``{\bf 自殺者数}が増えれば{\bf 幸福度}が低下する''という文脈からは「自殺者数」に対する負の重みを推定する．
一方で，ある順序基準と属性の対に対して，もし文脈が得られなかった場合には，
その属性に対する重みは0に近いという推測をする．
これらのアイデアは次節で説明する文脈誘導型学習によって具体化される．

\section{文脈誘導型学習}
本節ではラベル基準と特徴の文脈を活用する文脈誘導型学習という学習手法を提案する．
まず，分類問題に対する文脈誘導型学習を導入し，
次にランキング問題に対する拡張を行う．

入力はベクトルとラベルのペアの集合の集合である：
${\mathcal D} = \{D_k\}^K_{k = 1}$．
ただし， $D_k=\{({\mathbf x}_{k,i}, y_{k,i})\}^{N_k}_{i=1}$，
${\mathbf x}_{k,i} \in {\mathbb R}^M$, $y_{k,i} \in \{-1, +1\}$，
そして，$K$はラベル基準の総数である．
ラベル基準$l_k$はラベル$y_{k,i}$を決定するためのテキスト表現である．
例えば，${\mathbf x}_{k,i}$が都市の特徴を表し，
都市が大都市であれば$y_{k,i} = +1$，そうでなければ$y_{k,i} = -1$とする．
この場合，ラベル基準$l_k$は「大都市」となる．
例えば，${\mathbf x}_{k,i}$が文書の特徴を表し，
文書がスパムであれば$y_{k,i} = +1$，そうでなければ$y_{k,i} = -1$とする．
この場合，ラベル基準$l_k$は「スパム」となる．
ベクトルの$d$次元目の値はある特徴に対応し，その特徴の名前を$a_d$とする．
例えば，特徴名として「人口」や「面積」，「リンク数」などが考えられる．

文脈誘導型学習を適用するための要件は以下の3つである：
\begin{enumerate}
\setlength{\parskip}{0em}
\setlength{\itemsep}{0em}
\item ラベル基準$l_k$が言語で表現されている．
\item 特徴$A=\{a_d\}^{M}_{d=1}$が言語で表現されている．
\item ラベル基準と特徴の関係に関する文脈を含むコーパスが存在する．
\end{enumerate}
マルチタスク学習問題とは異なり，文脈誘導型学習ではタスクの類似性を仮定しない
（または，文脈誘導型学習の言葉で言えば，ラベル基準が類似する必要がない）．
また，全てのラベル基準および特徴が言語で表現されている必要もない．

分類問題は，各ラベル基準$k=1, \ldots, K$に対して$f_k({\mathbf x}_{k,i}) \simeq y_{k,i}$であるような関数$f_k$を学習する問題であると定式化できる．
文脈誘導型学習によってこの問題を解くために，
我々は線形関数$f_k({\mathbf x}_{k,i}) = {\mathbf w}_k^T{\mathbf x}_{k,i}$を用いる．
ラベル基準$l_k$と特徴$a_d \in A$の文脈を$c_{k, d}$とし，
以下のようにして${\mathbf w}_k$ を推定するために文脈を用いることができる：
\begin{eqnarray}
w_{k,d} = {\mathbf u}^T \phi(c_{k, d}) + v_{k, d},
\label{eq:context_function}
\end{eqnarray}
ただし，$w_{k,d}$は${\mathbf w}_k$の$d$次元目の値，$\phi$は文脈からベクトルへの特徴写像関数，
${\mathbf u}$はラベル基準に依存しない重みベクトルである．
上記の式は，あるラベル基準$l_k$と特徴$a_d$の重みがそれらの文脈である$c_{k, d}$と切片$v_{k, d}$から推定されることを意味する．
また，我々は$v_{k, d}$が「大きくない」ことを，つまり，$w_{k,d}$が$v_{k, d}$のみによって決定されるのでなく，${\mathbf u}^T \phi(c_{k, d})$によっても決定されることを期待する．
式\ref{eq:context_function}は正則化マルチタスク学習~\cite{evgeniou2004regularized}における$w_{k,d} = z_d + v_{k,d}$（$z_d$は複数タスクに共通の重み）を一般形である．
もし全ての文脈が同じであれば式\ref{eq:context_function}は正則化マルチタスク学習に還元される．
もし2つのラベル基準の文脈が似る，つまり，ラベル基準が類似するのであれば，
$w_{k,d}$はそれらのラベル基準に対して似た値となる．
この性質はいくつかのマルチタスク学習手法に似ている~\cite{jacob2009clustered,kumar2012learning}．
